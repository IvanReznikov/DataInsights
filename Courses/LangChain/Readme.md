# LangChain 101 Course
The course is still under development. Feedback and suggestions are appreciated.

Onepager:
- [LangChain Cheatsheet â€” All Secrets on a Single Page](https://medium.com/towards-artificial-intelligence/langchain-cheatsheet-all-secrets-on-a-single-page-8be26b721cde)

### 1. Introduction to LangChain
* What is LangChain?
* What are the components of LangChain?
* How do LangChain components work together?

Articles:
- [LangChain 101: Part 1. Building Simple Q&A App](https://pub.towardsai.net/langchain-101-part-1-building-simple-q-a-app-90d9c4e815f3)

Colab: https://colab.research.google.com/drive/1svn1BPfIiUL-82P4Rv_smoTPPyoeuWfa#scrollTo=gXypMluAjBME

### 2. Large Language Models
* What is the LLM?
* What are tokens?
* How to speak to the model directly?
* What are embeddings?
* How to use opensource alternatives to openai models?
* How to quantitize models to run in colab?
* Simple langchain integration
* How to launch a large language model CPU-only?
* How text is generated and what decoding strategies exist?
* How to finetune a model?
* How to use human feedback?

Articles:
- [LangChain 101: Part 2ab. All You Need to Know About (Large Language) Models](https://pub.towardsai.net/langchain-101-part-2ab-all-you-need-to-know-about-large-language-models-3512ae41dfc3)
- [LangChain 101: Part 2c. Fine-tuning LLMs with PEFT, LORA, and RL](https://pub.towardsai.net/langchain-101-part-2c-fine-tuning-llms-with-peft-lora-and-rl-5c9890ed0766)
- [LangChain 101: Part 2d. Fine-tuning LLMs with Human Feedback](https://pub.towardsai.net/langchain-101-part-2d-fine-tuning-llms-with-human-feedback-57769479d013)
- [How Does an LLM Generate Text?](https://pub.towardsai.net/how-does-an-llm-generate-text-fd9c57781217)
- [How to Fit Large Language Models in Small Memory: Quantization](https://pub.towardsai.net/how-to-fit-large-language-models-in-small-memory-quantization-e8c3981430b2)

Colab:
- [Models](https://colab.research.google.com/drive/1Z7ka-fOfF47P3tcpOPp-wBT9sMn3Ltnp?usp=sharing)
- [Text Generation and Decoding Strategies](https://colab.research.google.com/drive/14YZHDv7XyxRdSAjlJaaZRlBa-GYi9Ufu?usp=sharing)
- [Finetuning](https://colab.research.google.com/drive/1wPf47vMLYA33XQEGltH9E-ArzHDvWE6o?usp=sharing)
- [RLHF with Table Data](https://colab.research.google.com/drive/1n3QjpWLyXLFCRdf5fVwQ-KwwZeTF7uxI?usp=sharing)
- [RLHF with Label Studio and PPO](https://colab.research.google.com/drive/1lYWI2xoFYfNYTKB03OwWaZNNGu1PTLv6?usp=sharing)


### 3: Vectorstores and How to Work with Data
* How to use LLMs with your own data?
* What are data loaders?
* What are data splitters?
* What are embeddings and how to choose correct ones?
* What are indexes
* What are vectorstores and how to choose the one that fits you?
* How to build RAG pipelines?
* Tips and tricks on setting up RAG pipelines.
* RAG pipelines in production

Articles:
- [LangChain 101: Part 3a. Talking to Documents: Load, Split, and simple RAG with LCEL](https://pub.towardsai.net/langchain-101-part-3a-talking-to-documents-load-split-and-simple-rag-with-lcel-26b005ccb30a)
- [LangChain 101: Part 3b. Talking to Documents: Embeddings and Vectorstores](https://pub.towardsai.net/langchain-101-part-3b-talking-to-documents-embeddings-and-vectorstores-c37d460f1519)

Colab:
- [Loaders and Splitters](https://colab.research.google.com/drive/1Fjac0BUzY41DDYtbfApvWH5BUdQVX6q9?usp=sharing)
- [Embeddings and Vectorstores](https://colab.research.google.com/drive/1gZ8CfC0n2hNczYfoqynezDkDsMtTmEbi?usp=sharing)

  
### 4. Prompts
* What are prompts?
* How do prompts work?
* How to write effective prompts
* Examples of prompts

### 5. Memory
* What is memory?
* How does memory work?
* How to use memory in chains
* Examples of using memory in chains

### 6. Chains
* What is a chain?
* How to create a chain
* How to use chains to perform tasks
* Different types of chains

### 7. Agents and Tools
* What is the agent?
* What is the role of the agent?
* How to create an agent

### 8. Deployment
* How to deploy LangChain applications
* Where to deploy LangChain applications

### 9, 10. TBD
